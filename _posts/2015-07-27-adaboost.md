---
layout:     post
title:      "AdaBoost算法"
data: 2015-07-27 22:49:30
permalink:  adaboost.html
categories: 机器学习
tags: boost
excerpt: AdaBoost算法原理
mathjax: true
---

* content
{:toc}


## 算法
$\qquad$ 输入：训练数据集 $T=\\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\\}$，其中 $x_i\in\Bbb R^n,\quad y_i\in\\{-1,+1\\}$；弱学习算法；

$\qquad$ 输出：最终分类器 $G(x)$ .

$\qquad$ (1) 初始化训练数据的权值分布

$$D_1=(w_{11},\cdots,w_{1i},\cdots,w_{1N}),\quad w_{1i}=\frac 1 N,\quad i=1,2,\cdots,N$$

$\qquad$ (2) 对 $m=1,2,\cdots,M$

$\qquad\quad$ (a) 使用具有权值分布 $D_m$ 的训练数据集学习，得到基本分类器

$$G_m(x): \mathcal{X}\rightarrow\{-1,+1\} $$

$\qquad\quad$ (b) 计算 $G_m(x)$ 在训练数据集上的分类误差率

$$e_m=P(G_m(x_i)\neq y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\neq y_i)$$

$\qquad\quad$ (c) 计算 $G_m(x)$ 的系数

$$\alpha_m=\frac 1 2 \log\frac{1-e_m}{e_m}$$

这里的对数是自然对数 .

$\qquad\quad$ (d) 更新训练数据集的权值分布

$$D_{m+1}=(w_{m+1,1},\cdots,w_{m+1,i}\cdots,w_{m+1,N})$$

$$w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp(-\alpha_my_iG_m(x_i)),\quad i=1,2,\cdots,N$$

这里，$Z_m$ 是规范化因子

$$Z_m=\sum_{i=1}^Nw_{mi}\exp(-\alpha_my_iG_m(x_i))$$

它使 $D_{m+1}$ 成为一个概率分布 .

$\qquad$ (3) 构造基本分类器的线性组合

$$f(x)=\sum_{m=1}^M\alpha_mG_m(x)$$

得到最终分类器

$$G(x)=\text{sign}(f(x))=\text{sign}\left(\sum_{m=1}^M\alpha_mG_m(x)\right)$$
